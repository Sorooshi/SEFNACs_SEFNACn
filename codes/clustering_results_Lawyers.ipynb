{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.stats.ox.ac.uk/~snijders/siena/Lazega_lawyers_data.htm   data set taken from\n",
    "\n",
    "https://www.stats.ox.ac.uk/~snijders/siena/siena_datasets.htm    main page of SIENA\n",
    "\n",
    "https://scholar.google.com/citations?user=xqefLxQAAAAJ&hl=en&oi=sra  <<< Authors which provide the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML('''<script>\n",
    "# code_show=true; \n",
    "# function code_toggle() {\n",
    "#  if (code_show){\n",
    "#  $('div.input').hide();\n",
    "#  } else {\n",
    "#  $('div.input').show();\n",
    "#  }\n",
    "#  code_show = !code_show\n",
    "# } \n",
    "# $( document ).ready(code_toggle);\n",
    "# </script>\n",
    "# The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "# To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pickle\n",
    "import warnings\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx  as nx\n",
    "import evaluation as ev\n",
    "from sklearn import metrics\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "from pandas import DataFrame as df\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import OrderedDict, defaultdict\n",
    "import SEFNAC as sefnac \n",
    "from numpy.matlib import rand,zeros,ones,empty,eye\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 10, 17, 24, 31, 38, 45, 52, 59, 66]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivots = list(range(3, 71, 7))\n",
    "pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, linewidth=150, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Y(Yin):\n",
    "    \n",
    "    \"\"\"\n",
    "    input:\n",
    "    - Y: numpy array for Entity-to-feature\n",
    "    - ncf: Dict. the dict-key is the index of categorical variable V_l in Y, and dict-value is the number of \n",
    "    sub-categorie b_v (|V_l|) in categorical feature V_l.\n",
    "    \n",
    "    Apply Z-scoring, preprocessing by range, and Prof. Mirkin's 3-stage pre-processing methods.\n",
    "    return: Original entity-to-feature data matrix, Z-scored preprocessed matrix, 2-stages preprocessed matrix,\n",
    "    3-stages preprocessed matrix and their coresponding relative contribution\n",
    "    \"\"\" \n",
    "    \n",
    "    TY = np.sum(np.multiply(Yin, Yin))  # data scatter, the letter T stands for data scatter\n",
    "    TY_v = np.sum(np.multiply(Yin, Yin), axis=0)  # feature scatter \n",
    "    Y_rel_cntr = TY_v/TY  # relative contribution\n",
    "    \n",
    "    Y_mean = np.mean(Yin, axis=0)\n",
    "    Y_std = np.std(Yin, axis=0)\n",
    "    \n",
    "    Yz = (Yin - Y_mean) / Y_std # Z-score\n",
    "    TYz = np.sum(np.multiply(Yz, Yz))\n",
    "    TYz_v = np.sum(np.multiply(Yz, Yz), axis=0)\n",
    "    Yz_rel_cntr = TYz_v / TYz\n",
    "\n",
    "    scale_min_Y = np.min(Yin, axis=0); scale_max_Y = np.max(Yin, axis=0); rng_Y = scale_max_Y - scale_min_Y\n",
    "    \n",
    "    Yrng = (Yin - Y_mean)/ rng_Y  # 3 steps preprocessing (Range-without follow-up division)\n",
    "    TYrng = np.sum(np.multiply(Yrng, Yrng))\n",
    "    TYrng_v = np.sum(np.multiply(Yrng, Yrng), axis=0)\n",
    "    Yrng_rel_cntr = TYrng_v/TYrng\n",
    "    \n",
    "    # This section is not used for synthetic data, because no categorical data is generated.\n",
    "    Yrng_rs = deepcopy(Yrng) # 3 steps preprocessing (Range-with follow-up division)\n",
    "    for k, v in ncf.items(): \n",
    "        Yrng_rs[:, int(k)] = Yrng_rs[:, int(k)]/np.sqrt(int(v))  # : int(k)+ int(v)\n",
    "        \n",
    "#     Yrng_rs = (Y_rescale - Y_mean)/ rng_Y\n",
    "    TYrng_rs = np.sum(np.multiply(Yrng_rs, Yrng_rs))\n",
    "    TYrng_v_rs = np.sum(np.multiply(Yrng_rs, Yrng_rs), axis=0)\n",
    "    Yrng_rel_cntr_rs = TYrng_v_rs/TYrng_rs\n",
    "    \n",
    "    return Y_rel_cntr, Yz, Yz_rel_cntr, Yrng, Yrng_rel_cntr, Yrng_rs, Yrng_rel_cntr_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_P(Pin):\n",
    "\n",
    "    \"\"\"\n",
    "    input: Adjacency matrix\n",
    "    Apply Uniform, Modularity, Lapin preprocessing methods.\n",
    "    return: Original Adjanceny matrix, Uniform preprocessed matrix, Modularity preprocessed matrix, and \n",
    "    Lapin preprocessed matrix and their coresponding relative contribution\n",
    "    \"\"\" \n",
    "    N, V = Pin.shape\n",
    "    P_sum_sim = np.sum(Pin)\n",
    "    P_ave_sim = (np.sum(Pin) / (N*(V-1)))\n",
    "    cnt_rnd_interact = np.mean(Pin, axis=1) # constant random interaction \n",
    "    \n",
    "    \n",
    "    # Uniform method\n",
    "    Pu = Pin - cnt_rnd_interact\n",
    "    Pu_sum_sim = np.sum(Pu)\n",
    "    Pu_ave_sim = (np.sum(Pu) / (N*(V-1)))\n",
    "    \n",
    "    # Modularity method (random interaction)\n",
    "    P_row = np.sum(Pin, axis=0) \n",
    "    P_col = np.sum(Pin, axis=1)\n",
    "    P_tot = np.sum(Pin)\n",
    "    rnd_interact = np.multiply(P_row, P_col)/P_tot # random interaction formula\n",
    "    Pm = Pin - rnd_interact\n",
    "    Pm_sum_sim = np.sum(Pm)\n",
    "    Pm_ave_sim = (np.sum(Pm) /(N*(V-1)))\n",
    "    \n",
    "    # Lapin (Laplacian Inverse Transform)\n",
    "    # Laplacian\n",
    "    \n",
    "    r, c = Pin.shape\n",
    "    P_ = (Pin + Pin.T)/2\n",
    "    Pr = np.sum(P_, axis=1)\n",
    "    D = np.diag(Pr)\n",
    "    D = np.sqrt(D)\n",
    "    Di = LA.pinv(D)\n",
    "    L = eye(r) - Di@Pin@Di\n",
    "    \n",
    "    PL_sum_sim = np.sum(L)\n",
    "    PL_ave_sim = (np.sum(L)/(N*(V-1)))\n",
    "    \n",
    "    # pseudo-inverse transformation\n",
    "    L = (L + L.T)/2\n",
    "    M, Z = LA.eig(L)\n",
    "    ee = np.diag(M)\n",
    "    ind = list(np.nonzero(ee>0)[0])\n",
    "    Zn = Z[ind, ind]\n",
    "    Mn = np.asarray(M[ind])\n",
    "    Mi = np.asarray(1/Mn)\n",
    "    lapin =  Zn@Mi@Zn\n",
    "#     print(\"Zn:\", Zn.shape)\n",
    "#     print(\"Mi\", Mi.shape)\n",
    "#     print(\"Zn.T:\", Zn.T.shape)\n",
    "#     lapin =  Zn@Mi@Zn\n",
    "    \n",
    "    return P_sum_sim, P_ave_sim, Pu, Pu_sum_sim, Pu_ave_sim, Pm, Pm_sum_sim, Pm_ave_sim, L, PL_sum_sim, PL_ave_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_adjacency_matrix(G, title=\"\", node_order=None, partitions=[], colors=[], partitions_name=[]):\n",
    "\n",
    "    \"\"\"\n",
    "    - G is a netorwkx graph\n",
    "    - node_order (optional) is a list of nodes, where each node in G\n",
    "          appears exactly once\n",
    "    - partitions is a list of node lists, where each node in G appears\n",
    "          in exactly one node list\n",
    "    - colors is a list of strings indicating what color each\n",
    "          partition should be\n",
    "    If partitions is specified, the same number of colors needs to be\n",
    "    specified.\n",
    "    \"\"\"\n",
    "    \n",
    "    adjacency_matrix = nx.to_numpy_matrix(G, dtype=np.bool, nodelist=node_order)\n",
    "\n",
    "    #Plot adjacency matrix \n",
    "    fig = plt.figure(figsize=(5, 5)) # in inches\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.imshow(adjacency_matrix,\n",
    "               cmap=\"cool_r\",  # spring, cool\n",
    "               interpolation=\"None\",\n",
    "               origin='lower',)  # extent=[0, 70, 0, 70] to chane the x and y axises value\n",
    "\n",
    "    # The rest is just if you have sorted nodes by a partition and want to\n",
    "    # highlight the module boundaries\n",
    "    assert len(partitions) == len(colors)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    for partition, color in zip(partitions, colors):\n",
    "        rec_cord = 0\n",
    "        txt_cord = 10\n",
    "        p = 0    \n",
    "        for module in partition:\n",
    "            ax.add_patch(patches.Rectangle((rec_cord, rec_cord),\n",
    "                                          len(module), # Width\n",
    "                                          len(module), # Height\n",
    "                                          facecolor=\"none\",\n",
    "                                          edgecolor=color,\n",
    "                                          linewidth=\"1\",\n",
    "                                          )),\n",
    "            if len(v) != 0:\n",
    "                ax.text(txt_cord, txt_cord, partitions_name[p], ha=\"center\", va=\"center\", color=\"k\", fontsize=16,)\n",
    "            rec_cord += len(module)\n",
    "            txt_cord += len(module)\n",
    "            p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_several_adjacencies(Graph, dict_of_features, rows, cols, ind, size_of_figures=(20, 10)):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is a very dirty extention of draw_adjacency_matrix funtion to be able\n",
    "    to see all of the matrices all together into one figure using plt.subplots().\n",
    "    \"\"\"\n",
    "    fi = 0\n",
    "    fig, ax = plt.subplots(figsize=size_of_figures) \n",
    "\n",
    "    for k, v in dict_of_features.items():\n",
    "        gnfl = group_feature_to_lists(ELattr[:, fi])  # Grouping Nodes according to the Feature to Lists\n",
    "        nodes_gnfl_ordered = [node for group in gnfl for node in group]  # Create a list of all nodes sorted by coresponding feature\n",
    "\n",
    "        adjacency_matrix = nx.to_numpy_matrix(Graph, dtype=np.bool, nodelist=nodes_gnfl_ordered)\n",
    "\n",
    "        plt.subplot(rows, cols, ind)\n",
    "        plt.title(k, fontsize=50)\n",
    "\n",
    "        #Plot adjacency matrix \n",
    "        plt.imshow(adjacency_matrix,\n",
    "                   cmap=\"cool_r\",  # spring, cool\n",
    "                   interpolation=\"None\",\n",
    "                   origin='lower',)  # extent=[0, 70, 0, 70] to chane the x and y axises value\n",
    "\n",
    "        # The rest is just if you have sorted nodes by a partition and want to\n",
    "        # highlight the module boundaries\n",
    "        colors=[\"black\"]\n",
    "\n",
    "        assert len([gnfl]) == len(colors)\n",
    "        ax = plt.gca()\n",
    "        p = 0\n",
    "        for partition, color in zip([gnfl], colors):\n",
    "            rec_cord = 0\n",
    "            txt_cord = 10\n",
    "            for module in partition:\n",
    "                ax.add_patch(patches.Rectangle((rec_cord, rec_cord),\n",
    "                                              len(module), # Width\n",
    "                                              len(module), # Height\n",
    "                                              facecolor=\"none\",\n",
    "                                              edgecolor=color,\n",
    "                                              linewidth=\"3\",))\n",
    "                if len(v) != 0:\n",
    "                    ax.text(txt_cord, txt_cord, v[p], ha=\"center\", va=\"center\", color=\"k\", fontsize=38,)\n",
    "                rec_cord += len(module)\n",
    "                txt_cord += len(module)\n",
    "                p += 1\n",
    "        ind += 1\n",
    "        fi += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_feature_to_lists(input_feature):\n",
    "    by_feature_value = defaultdict(list)\n",
    "    for node_index, attribute_value in enumerate(input_feature):\n",
    "        by_feature_value[attribute_value].append(node_index)\n",
    "    return by_feature_value.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_SEFNAC(Yi, Pi, ro_g, ro_f, data_name):\n",
    "    \"\"\"\n",
    "    input: entity-to-feature matrix Yi, Adjacency matrix Pi, ro_f, ro_g trade-off factor\n",
    "    between data matrices, data_name.\n",
    "    applies simultanouse anamalouse (network) clustering algorithm, sa(n)c.\n",
    "    return: clustering_results as it is appeared and sorted clustering results based on the number\n",
    "    of members in each cluster in ascending order.\n",
    "    \"\"\"\n",
    "    out_ms, out_ms_sorted, contingency_table = OrderedDict([]), OrderedDict([]), OrderedDict([])\n",
    "    N, V = Yi.shape\n",
    "    \n",
    "    path = data_name + \"-\" + str(ro_g) + \"-\" + str(ro_f)\n",
    "    print(\"new path:\", path,)\n",
    "\n",
    "    for pivot in pivots:\n",
    "        path_ = path + \"-pivot:\" + str(pivot)\n",
    "        print(\"pivot:\", pivot)\n",
    "        \n",
    "        out_ms[path_] = sefnac.run_ANomalous_Cluster(pivot=pivot, Y=Yi, P=Pi, rho_f=ro_f, rho_g=ro_g)\n",
    "        length_labels_pred, sorted_out_ms = sefnac.sorting_results(out_ms[path_])\n",
    "        out_ms_sorted[path_] = sorted_out_ms.values()\n",
    "\n",
    "    result_path = data_name + \"-\" + \"-ro-g=\" + str(ro_g) + \"-ro_f=\" + str(ro_f)\n",
    "    return out_ms, out_ms_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QScaD(Y_, P_, ro_f, ro_g, results):\n",
    "    \n",
    "    \"\"\"\n",
    "    input: the original entity-to-feature matrix and the one which  is used for clustering process.\n",
    "    The original adjacency matrix and the one which is used for the clustering process.\n",
    "    And finally, the sorted clustering results dictionary.\n",
    "    computes and prints out grand means of data matrices, \"features mean\" in each cluster, \n",
    "    cluster contribution, relative_difference\n",
    "    return: Dict of Dict \n",
    "    \"\"\"\n",
    "    \n",
    "    QScaD = {}\n",
    "    Y = np.multiply(Y_, ro_f)\n",
    "    P = np.multiply(P_, ro_g)\n",
    "    for k, v in results.items():\n",
    "        QScaD[k] = {}\n",
    "        TY = np.sum(np.multiply(Y, Y))\n",
    "#         Tv = np.mean(np.multiply(Y, Y), axis=0)\n",
    "        Tv = np.sum(np.multiply(Y, Y), axis=0)\n",
    "        PSs = np.sum(np.multiply(P, P))\n",
    "        Cent, Ck, Bcnt, Bk, Rcnt, Rk, Dcnt, Dk, RCI, Rck, Bv, YExpl, YUnExpl, PExpl, \\\n",
    "        PUnExpl, = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "        \n",
    "        i = 1\n",
    "        for indices in v:\n",
    "            \n",
    "            Cent[str(i)] = np.mean(Y[indices, :], axis=0)\n",
    "            Ck[str(i)] = np.mean(Y[indices, :], )\n",
    "            B_kv = np.divide(len(indices) * np.mean(np.power(Y[indices, :], 2), axis=0), 1)  # B_kv\n",
    "            Bcnt[str(i)] = B_kv\n",
    "            Bk[str(i)] = np.sum(B_kv)  # B_k+\n",
    "            \n",
    "            B_kRv = np.divide(B_kv, Tv)  # B(k|v) \n",
    "            Rcnt[str(i)] = B_kRv\n",
    "            Rk[str(i)] = np.sum(B_kRv)\n",
    "            \n",
    "            G_kRv = np.subtract(B_kRv, np.divide(np.sum(B_kRv), TY))  # g(K|v)\n",
    "            Dcnt[str(i)] = G_kRv\n",
    "            Dk[str(i)] = np.sum(G_kRv)\n",
    "            \n",
    "            Q_kRv = np.subtract(np.divide(np.multiply(TY, B_kv), np.multiply(Tv, np.sum(B_kv))), 1)  # q(k|v)\n",
    "            RCI[str(i)] = Q_kRv\n",
    "            Rck[str(i)] = np.sum(Q_kRv)\n",
    "            \n",
    "            CCo = np.reshape([P[i, j] for i in indices for j in indices], (len(indices), -1)) # Cluster Co-occure\n",
    "            CSs = np.sum(CCo)  # Cluster Similarity Sum \n",
    "            Lambda = np.divide(CSs, len(indices)**2)\n",
    "            \n",
    "            PExpl[str(i)] = Lambda * CSs\n",
    "            PUnExpl[str(i)] = PSs - (Lambda * CSs)\n",
    "            i += 1\n",
    "        \n",
    "        QScaD[k]['Cent'] = Cent \n",
    "        QScaD[k]['Ck'] = Ck\n",
    "        QScaD[k]['Bcnt'] = Bcnt\n",
    "        QScaD[k]['Rcnt'] = Rcnt\n",
    "        QScaD[k]['Dcnt'] = Dcnt\n",
    "        QScaD[k]['RCI'] = RCI\n",
    "        \n",
    "        QScaD[k]['Bk'] = Bk\n",
    "        QScaD[k]['Rk'] = Rk\n",
    "        QScaD[k]['Dk'] = Dk\n",
    "        QScaD[k]['Rck'] = Rck\n",
    "\n",
    "        QScaD[k]['PExpl'] = PExpl\n",
    "        QScaD[k]['PUnExpl'] = PUnExpl\n",
    "        \n",
    "        \n",
    "    return QScaD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate the impact of different preprocessoing methods on feature data and on graph's data. These methods are :\n",
    "\n",
    "* For feature data: Z-scoring, renage scale, and 3-stage with follow up rescale.\n",
    "* For Graph data:Uniform, Modularity and Lapin\n",
    "\n",
    "\n",
    "3-stage preprocessing technique normalized the data contribution and save the shape of data scatter.\n",
    "\n",
    "We also investigate the impact of $\\rho_{f}$ and $\\rho_{g}$. \n",
    "\n",
    "To evaluate the clustering results we use Four different methods.\n",
    "\n",
    "1) Contribution and relative contribution tables\n",
    "\n",
    "2) Adjusted Rand index ARI\n",
    "\n",
    "3) Contingency table \n",
    "\n",
    "At first step, we determine which set of features contribute each cluster, we also show the explain and unexplain part of graph data in each cluster. However, since the value of the explain and unexplain parts of graph data are not intuitivem to show the effect of graph data we recolor the network regarding the each cluster. \n",
    "\n",
    "We also use ARI, and the contingency as another set of criteria. We create the ground truth based on status and office location of laywers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELfriend: <class 'numpy.ndarray'> (71, 71)\n",
      "ELattr: <class 'numpy.ndarray'> (71, 8) (71, 7)\n"
     ]
    }
   ],
   "source": [
    "ELfriend = np.loadtxt(\"../../data/Lawyers/ELfriend.dat\")\n",
    "ELattr = np.loadtxt(\"../../data/Lawyers/ELattr.dat\")\n",
    "Yin = ELattr[:, 1:].copy()\n",
    "Pin = ELfriend.copy()\n",
    "\n",
    "N, V = Yin.shape\n",
    "\n",
    "print(\"ELfriend:\", type(ELfriend), ELfriend.shape)\n",
    "print(\"ELattr:\", type(ELattr), ELattr.shape, Yin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"sen, sta, gen, off, yif, age, pra, law\"]  \n",
    "\n",
    "features_dict = {\"Seniority\": \"\", \"Status\": [\"Partner\", \"Associate\"], \"Gender\": [\"Male\", \"Female\"], \n",
    "                 \"Office\": [\"Boston\", \"Hartford\", \"Providence\"], \"Years in Firm\": \"\", \"Age\": \"\", \n",
    "                 \"practice\": [\"Litigation\", \"Corporate\"], \"Law School\": [\"Harvard, Yale\", \"ucon\", \"Other\"]} \n",
    "\n",
    "print(\"Features are:\")\n",
    "print(\"Seniority, status (1=partner; 2=associate), gender (1=man; 2=woman),\") \n",
    "print(\"office (1=Boston; 2=Hartford; 3=Providence), years with the firm, age,\") \n",
    "print(\"practice (1=litigation; 2=corporate), law school (1: harvard, yale; 2: ucon; 3: other)\")\n",
    "GP = nx.from_numpy_matrix(ELfriend)\n",
    "draw_adjacency_matrix(G=GP, title=\"Original Adjacency Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_nx_org = {}\n",
    "for i in range(N):\n",
    "    labels_nx_org[i] = i\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(10, 5.5))\n",
    "pos_org = nx.kamada_kawai_layout(GP)\n",
    "\n",
    "nx.draw_networkx(G=GP, pos=pos_org, nodelist=list(range(0,N)),\n",
    "                 labels=labels_nx_org, alpha=0.9, width=.3, node_color='lime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can easily perceive no specific pattern can be detected in the network structure.  In the next step, we sort the adjacency matrix regarding each feature to review the relation between each feature and the graph's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = plt.hist(Yin[:, 3])\n",
    "plt.title(\"Histogram of Years with the Firm\")\n",
    "plt.show()\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 years with the firm is chosen as a threshold for categorizing this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_ = plt.hist(Yin[:, 4])\n",
    "plt.title(\"Histogram of Lawyers Age\")\n",
    "plt.show()\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age of 40, between 40 and 50 years old and greater than 50 years old Lawyers, are the three categories for age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Seniority\", \"Status\", \"Gender\", \"Office\", \"Years in Firm\", \"Age\", \"practice\", \"Law School\"]\n",
    "\n",
    "Yc = np.zeros([N, 7])\n",
    "\n",
    "for i in range(N):\n",
    "    for v in range(V):\n",
    "        if v == 3 and Yin[i, v] < 10:  \n",
    "            Yc[i, v] = 1\n",
    "        elif v == 3 and  10 <= Yin[i, v] < 20:\n",
    "            Yc[i, v] = 2\n",
    "        elif v == 3 and  20 <= Yin[i, v]:\n",
    "            Yc[i, v] = 3\n",
    "        elif v == 4 and Yin[i, v] < 40:\n",
    "            Yc[i, v] = 1\n",
    "        elif v == 4 and  40 <= Yin[i, v] < 50:\n",
    "            Yc[i, v] = 2\n",
    "        elif v == 4 and  50 <= Yin[i, v]:\n",
    "            Yc[i, v] = 3\n",
    "        else:\n",
    "            Yc[i, v] = Yin[i, v]\n",
    "\n",
    "Yc_unique = np.zeros([N, V])\n",
    "interval = 0\n",
    "for v in range(V):\n",
    "    for i in range(N):\n",
    "        if v == 0:\n",
    "            Yc_unique[i, v] = Yc[i, v]\n",
    "        else:\n",
    "            Yc_unique[i, v] = Yc[i, v] + interval  # even without interval it's ok\n",
    "    if v != 0:        \n",
    "        interval += max(list(set(Yc[:, v])))\n",
    "\n",
    "print(\"Mod Y Dim:\", Yc_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(categories='auto', sparse=False)\n",
    "Y_oneHot_final = enc.fit_transform(Yc)  # oneHot encoding\n",
    "print(Y_oneHot_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix of categorical and quatitative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_oneHot_1 = enc.fit_transform(Yin[:, :3])\n",
    "# Y_oneHot_2 = enc.fit_transform(Yin[:, 5:])\n",
    "# Y_oneHot_mix = np.concatenate([Y_oneHot_1, Yin[:, 3:5], Y_oneHot_2, ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SEFNAC on Yrang and Pm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncf = {'1':2, '2':2, '3':3, '6':2, '7':3}  # ORIGINAL Y\n",
    "ncf = {'0':2, '1':2, \n",
    "       '2':2, '3':2, \n",
    "       '4':3, '5':3, '6':3,\n",
    "       '9':2, '10':2,\n",
    "       '11':3, '12':3, '13':3\n",
    "       }\n",
    "Y_rel_cntr, Yz, Yz_rel_cntr, Yrng, Yrng_rel_cntr, Yrng_rs, Yrng_rel_cntr_rs = preprocess_Y(Yin=Y_oneHot_final)  # Y_oneHot_mix >> for Mixed!\n",
    "print(\"Features: seniority, status (1=partner; 2=associate), gender (1=man; 2=woman), office (1=Boston; 2=Hartford; 3=Providence), years with the firm, age, practice (1=litigation; 2=corporate), law school (1: harvard, yale; 2: ucon; 3: other)\")\n",
    "print(\" \")\n",
    "print(\"Y relative cntr              :\", Y_rel_cntr, sum(Y_rel_cntr))\n",
    "print(\"Z-score relative cntr        :\", Yz_rel_cntr, sum(Yz_rel_cntr))\n",
    "print(\"Yrng relative cntr           :\", Yrng_rel_cntr, '%.2f' % sum(Yrng_rel_cntr))\n",
    "print(\"rescaled Yrng relative cntr  :\", Yrng_rel_cntr_rs, '%2.f' % sum(Yrng_rel_cntr_rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ncf = {'1':2, '2':2, '3':3, '6':2, '7':3}  # ORIGINAL Y\n",
    "\n",
    "# ncf_all = {'0':2, '1':2,\n",
    "#            '2':2, '3':2, \n",
    "#            '4':3, '5':3, '6':3,\n",
    "#            '7':3, '8':3,\n",
    "#            '9':2, '10':2,\n",
    "#            '11':3, '12':3, '13':3\n",
    "#        }\n",
    "\n",
    "# Y_rel_cntr_all, Yz_all, Yz_rel_cntr_all, Yrng_all, Yrng_rel_cntr_all, Yrng_rs_all, Yrng_rel_cntr_rs_all = preprocess_Y(Yin=Y_oneHot)\n",
    "# print(\"Features: seniority, status (1=partner; 2=associate), gender (1=man; 2=woman), office (1=Boston; 2=Hartford; 3=Providence), years with the firm, age, practice (1=litigation; 2=corporate), law school (1: harvard, yale; 2: ucon; 3: other)\")\n",
    "# print(\" \")\n",
    "# print(\"Y relative cntr              :\", Y_rel_cnt_all, sum(Y_rel_cntr_all))\n",
    "# print(\"Z-score relative cntr        :\", Yz_rel_cntr_all, sum(Yz_rel_cntr_all))\n",
    "# print(\"Yrng relative cntr           :\", Yrng_rel_cntr_all, '%.1f' % sum(Yrng_rel_cntr_all))\n",
    "# print(\"rescaled Yrng relative cntr  :\", Yrng_rel_cntr_rs_all, '%.1f' % sum(Yrng_rel_cntr_rs_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_sum_sim, P_ave_sim, Pu, Pu_sum_sim, Pu_ave_sim, Pm, Pm_sum_sim, Pm_ave_sim, Pl, Pl_sum_sim, Pl_ave_sim = preprocess_P(ELfriend)  # PL, PL_rel_cntr \n",
    "\n",
    "# print(\"sum similarity of P:\", P_sum_sim)\n",
    "print(\"Ave similarity of P:\", '%.2f' % P_ave_sim)\n",
    "\n",
    "print(\"sum similarity of Pu:\", '%.2f' % Pu_sum_sim)\n",
    "print(\"Ave similarity of Pu:\", '%.2f' % Pu_ave_sim)\n",
    "\n",
    "print(\"sum similarity of Pm:\", '%.2f' % Pm_sum_sim)\n",
    "print(\"Ave similarity of Pm:\", '%.2f' % Pm_ave_sim)\n",
    "\n",
    "print(\"sum similarity of Pl:\", '%.2f' % Pl_sum_sim)\n",
    "print(\"Ave similarity of Pl:\", '%.2f' % Pl_ave_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The impact of applying different preprocessing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can easily perceive applying different preprocessing method does not affect the clustering results considerably. Taking a look at the values of ARI for different cases as well as comparing the contingency tables can prove this claim. However, they can slightly improve manifesting the role of each feature contribution in clustering preprocess. And this is why in the rest of this report we use the case which the features' data is preprocessed by utilizing the 3-stage method and the graph's data is preprocessed utilizing Modularity method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPm = nx.from_numpy_matrix(Pm)\n",
    "GPu = nx.from_numpy_matrix(Pu)\n",
    "GLa = nx.from_numpy_matrix(Pl)\n",
    "# draw_adjacency_matrix(GP, title=\"Without normalization\")\n",
    "# draw_adjacency_matrix(GPm, title=\"Normalized with Modulirty\")\n",
    "# draw_adjacency_matrix(GPu, title=\"Normalizad with uniform method\")\n",
    "# draw_adjacency_matrix(GLa, title=\"Normalizad with Lapin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting the original Adjanceny Matrix regarding different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_several_adjacencies(Graph=GP, dict_of_features=features_dict, rows=2, cols=4, ind=1, size_of_figures=(50, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By sorting the adjancency matrix redarding each feature and reviewing the paper which introduces the data set, we create the ground truth using status feature and office location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a ground Truth based on Status and Office location (features)\n",
    "#### Sorted Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = []\n",
    "for i in range(N):\n",
    "    tmp = ELattr[i, [1, 3]].tolist()\n",
    "#     print(tmp, type) \n",
    "    if tmp not in unique_labels:\n",
    "        unique_labels.append(tmp)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 1\n",
    "status = set(ELattr[:, 1])  # sorted(set(ELattr[:, 1]))\n",
    "office = set(ELattr[:, 3]) \n",
    "r, c = ELattr.shape\n",
    "all_possible_labels = list(itertools.product(status, office))\n",
    "\n",
    "r, c = ELattr.shape\n",
    "labels_true = []\n",
    "labels_true_final = np.zeros(N)\n",
    "labels_true_sian = []\n",
    "\n",
    "\n",
    "for lb in all_possible_labels:\n",
    "    tmp_labels, tmp_sian = [], []\n",
    "    at_least_one_label = False\n",
    "    for i in range(N):\n",
    "        if ELattr[i, 1] == lb[0] and ELattr[i, 3] == lb[1]:\n",
    "            at_least_one_label = True\n",
    "            tmp_labels.append(label)\n",
    "            labels_true_final[i] = label\n",
    "    if at_least_one_label is True:\n",
    "        label += 1\n",
    "    if len(tmp_labels) != 0:\n",
    "        labels_true.append(tmp_labels)\n",
    "        labels_true_sian.append(tmp_sian)\n",
    "        \n",
    "labels_true_final = labels_true_final.tolist()\n",
    "len(labels_true),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"labels_true_final_LAWYERS.npy\", labels_true_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SEFNAC algorithm on not preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ro_g = 1 # trade-off factor for interaction between entity-to-feature matrix and proximity\n",
    "ro_f = 1 # trade-off factor for interaction between entity-to-feature matrix and proximity\n",
    "\n",
    "ro_g_ = 2 # trade-off factor for interaction between entity-to-feature matrix and proximity\n",
    "ro_f_ = 0.2 # trade-off factor for interaction between entity-to-feature matrix and proximity\n",
    "\n",
    "ro_g__ = 0.2 # trade-off factor for interaction between entity-to-feature matrix and proximity\n",
    "ro_f__ = 2 # trade-off factor for interaction between entity-to-feature matrix and proximity\n",
    "\n",
    "Y_to_use = Yrng # Y_oneHot1\n",
    "P_to_use = Pu\n",
    "\n",
    "MS, _ = apply_SEFNAC(Yi=Y_to_use, Pi=P_to_use, ro_g=ro_g, ro_f=ro_f, data_name=\"ELfriend\")\n",
    "# MS_, MSS_ = apply_SEFNAC(Yi=Y_oneHot_final, Pi=Pin, ro_g=ro_g_, ro_f=ro_f_, data_name=\"ELfriend\")\n",
    "# MS__, MSS__ = apply_SEFNAC(Yi=Y_oneHot_final, Pi=Pin, ro_g=ro_g_, ro_f=ro_f_, data_name=\"ELfriend\")\n",
    "\n",
    "\n",
    "# T = QScaD(Y_=Yrng_rs, P_=Pm, ro_f=ro_f, ro_g=ro_g, results=MSS)\n",
    "# T_ = QScaD(Y_=Yrng_rs, P_=Pm, ro_f=ro_f_, ro_g=ro_g_, results=MSS_)\n",
    "# T__ = QScaD(Y_=Yrng_rs, P_=Pm, ro_f=ro_f__, ro_g=ro_g__, results=MSS__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjusted Rand index ARI:\n",
    "\n",
    "Given the knowledge of the ground truth class assignments, as it is described previously, and the clustering algorithm assignments, ARI is a function that measures the similarity of the two assignments, ignoring permutations. Random (uniform) label assignments have a ARI score close to 0.0 for any value of n_clusters and n_samples. Bounded range [-1, 1]: negative values are bad (independent labelings), similar clusterings have a positive ARI, 1.0 is the perfect match score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ari_val, nmi_val, num_clust = [], [], []\n",
    "for k, v in MS.items():\n",
    "    if int(k.split(\":\")[-1]) in pivots:\n",
    "        labels_pred = np.zeros([N])\n",
    "        ind = 1\n",
    "        for kk, vv in v.items():\n",
    "            for vvv in vv:\n",
    "                labels_pred[vvv] = ind\n",
    "                num_clust\n",
    "            ind += 1\n",
    "        num_clust += [ind-1]\n",
    "        ari = metrics.adjusted_rand_score(labels_true=labels_true_final, labels_pred=labels_pred)\n",
    "        ari_val.append(ari)\n",
    "        ami = metrics.adjusted_mutual_info_score(labels_true=labels_true_final, labels_pred=labels_pred)\n",
    "        nmi = metrics.normalized_mutual_info_score(labels_true=labels_true_final, labels_pred=labels_pred)\n",
    "        nmi_val.append(nmi)\n",
    "        print(\"results:\", k, \"ari:\", '%.2f' % ari, \"nmi:\", '%.2f' % nmi,)  # \"ami:\", '%.2f' % ami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_val = np.asarray(ari_val)\n",
    "nmi_val = np.asanyarray(nmi_val)\n",
    "num_clust = np.asanyarray(num_clust)\n",
    "\n",
    "print(\"   ARI   \", \"    NMI  \", \"  NUM_CLUST\")\n",
    "print(\"MEAN  STD\", \" MEAN STD\", \"  MEAN STD\")\n",
    "print(\"%.2f\" % np.mean(ari_val), \"%.2f\" % np.std(ari_val), \"%.2f\" % np.mean(nmi_val), \"%.2f\" % np.std(nmi_val), \"%.2f\" % np.mean(num_clust), \"%.2f\" % np.std(num_clust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all the values are one hot encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal weight for graph and feature data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To Investigate the clustering results regarding the features run the below cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in pivots:\n",
    "#     if i == np.argmax(ari_val):\n",
    "#         print(\"features:       \", features[1:])\n",
    "#         nc = 1\n",
    "#         print(\"features in:    \", features[0].split(\", \")[1:])\n",
    "#         for k, v in T['ELfriend-1-1-pivot:' + str(i)]['Cent'].items():\n",
    "#             print(\"pivot:\", i)\n",
    "#             print('cluster:', nc, 'Cent:', T['ELfriend-1-1-pivot:' + str(i)]['Cent'][str(nc)], '%.2f' % T['ELfriend-1-1-pivot:' + str(i)]['Ck'][str(nc)],)\n",
    "#             print('cluster:', nc, 'Bcnt:', T['ELfriend-1-1-pivot:' + str(i)]['Bcnt'][str(nc)], '%.2f' % T['ELfriend-1-1-pivot:' + str(i)]['Bk'][str(nc)],)\n",
    "# #             print('cluster:', nc, 'Rcnt:', T['ELfriend-1-1-pivot:' + str(i)]['Rcnt'][str(nc)], '%.2f' % T['ELfriend-1-1-pivot:' + str(i)]['Rk'][str(nc)],)\n",
    "# #             print('cluster:', nc, 'Dcnt:', T['ELfriend-1-1-pivot:' + str(i)]['Dcnt'][str(nc)], '%.2f' % T['ELfriend-1-1-pivot:' + str(i)]['Dk'][str(nc)])\n",
    "#             print('cluster:', nc, 'RCI: ', T['ELfriend-1-1-pivot:' + str(i)]['RCI' ][str(nc)], '%.2f' % T['ELfriend-1-1-pivot:' + str(i)]['Rck'][str(nc)])\n",
    "#             print('cluster:', nc, 'P-Expl:' ,'%.2f' % T['ELfriend-1-1-pivot:' + str(i)]['PExpl'][str(nc)], 'P-UnExpl:', '%.2f' % T['ELfriend-1-1-pivot:' + str(i)]['PUnExpl'][str(nc)])\n",
    "#             print(\" \")\n",
    "#             nc += 1\n",
    "#         print('***************************************************************************')\n",
    "#         print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = ['fuchsia','green', 'blue', 'aqua', 'lime', 'silver', 'red',\n",
    "#           'grey', 'maroon', 'purple', 'yellow', 'green', 'lime', 'olive', \n",
    "#           'yellow', 'navy', 'aqua', 'black', 'silver', 'grey', 'maroon', 'olive', 'teal',]\n",
    "# shapes = ['s', 'o', '^', '>', 'v', '<', 'd', 'p', 'h', '8']\n",
    "\n",
    "# for k, v in MS.items():\n",
    "#     if int(k.split(\":\")[-1]) == np.argmax(ari_val):\n",
    "#         print(\"number of clusters:\", len(v))\n",
    "#         s = 1\n",
    "#         nodes_size = []\n",
    "#         colors_map = []\n",
    "#         nodes_list = []\n",
    "#         shapes_map = []\n",
    "#         for kk, vv in v.items():\n",
    "#             nodes_list += vv\n",
    "#             for vvv in vv:\n",
    "#                 nodes_size += [s*100]\n",
    "#                 colors_map += [colors[s-1]] \n",
    "#                 shapes_map += shapes[s-1]\n",
    "#             s += 1\n",
    "#         plt.figure(figsize=(13, 6.5))\n",
    "# #         pdot = nx.drawing.nx_pydot.to_pydot(GP)\n",
    "# #         for i, node in enumerate(pdot.get_nodes()):\n",
    "# #             node.set_label(\"n%d\" % i)\n",
    "# #             node.set_shape(shapes_map[i-1])\n",
    "# #             node.set_fillcolor(colors_map[i-1])\n",
    "# #             node.set_color(colors_map[i])\n",
    "\n",
    "# #         for i, edge in enumerate(pdot.get_edges()):\n",
    "# #             edge.set_color(colors_map[1])\n",
    "\n",
    "        \n",
    "#         nx.draw_networkx(G=GP, pos=pos_org, nodelist=nodes_list,  # list(range(0,N))\n",
    "#                          labels=labels_nx_org, alpha=0.9, width=.1, \n",
    "#                          node_color=colors_map, \n",
    "#                          node_size=nodes_size)\n",
    "        \n",
    "#         plt.title(\"cluster results:\" + k, fontsize=20)\n",
    "#         plt.show()\n",
    "# #         png_path = k + \".png\"\n",
    "# #         pdot.write_png(png_path)\n",
    "#         print(\"*************************************************************************************************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating the clustering results for the case of not preprocessed data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contingency Tables For Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.plot(ari_val)\n",
    "plt.xlabel(\"pivots\")\n",
    "plt.ylabel(\"ARI\")\n",
    "plt.grid(True)\n",
    "# plt.legend(\"ARI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in MS.items():\n",
    "    if int(k.split(\":\")[-1]) == np.argmax(ari_val):\n",
    "        \n",
    "        labels_pred = []\n",
    "        ind = 1\n",
    "        for kk, vv in v.items():\n",
    "            for vvv in vv:\n",
    "                labels_pred.append(ind)\n",
    "            ind += 1\n",
    "\n",
    "        sk_contingency_matrix = contingency_matrix(labels_true=labels_true_final, \n",
    "                                                   labels_pred=labels_pred, eps=0.001)\n",
    "\n",
    "        row_cont, col_cont = sk_contingency_matrix.shape\n",
    "        sk_contingency_table = np.zeros([row_cont + 2, col_cont + 1])\n",
    "        rows, cols = sk_contingency_table.shape\n",
    "\n",
    "        # adding marginal row and columns to convert the contingency matrix to contingency table                                                  \n",
    "        sk_contingency_table[1:-1, 0:-1] = sk_contingency_matrix\n",
    "\n",
    "#         marginal row values (Nt+)                                                                                                               \n",
    "#         for row in range(rows):\n",
    "#             sk_contingency_table[row, -1] = int(sum(sk_contingency_table[row, :]))\n",
    "\n",
    "        # marginal column values (N+u)                                                                                                            \n",
    "        for col in range(cols):\n",
    "            \n",
    "            sk_contingency_table[-1, col] = int(sum(sk_contingency_table[:, col]))\n",
    "#             if 0 < col < cols-1:\n",
    "#                 print(col,  len(labels_true[col-1]))\n",
    "#                 sk_contingency_table[0, col] = len(labels_true[col-1])\n",
    "\n",
    "        print(sk_contingency_table)\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis\n",
    "\n",
    "#### Note:\n",
    "\n",
    "* The Description in this section may vary in each run, however, still it could be a good sample of analyzing the clustering results with the help Quetelet Relative index and graph structure\n",
    "\n",
    "Description:\n",
    "\n",
    "A closer look at the entity-to-feature data matrix of the first cluster, reveals that all the members of this cluster have the same status, that is, they work in associate status. Gender does not provide any specific information. Moreover, all members of this cluster are graduated from Ucon or other Universities except entity number 37. By taking into account the graph's data (nodes with fuchsia) and looking at the links in this cluster two important facts can be understood. Firstly, all these entities are highly connected with each other. Secondly, entity number 37, has a very strong connection with all the members of this cluster. (Regarding the years he spent in the firm and his connection it seems as if he is a person in charge in among the lawyer of this cluster.) \\qed.\n",
    "\n",
    "\n",
    "The most contributing features in the second cluster are \"age\" and \"years in the firm\". The average age of the member of the cluster is 52.4 and the average years in the firm is 21. Besides that, the status all the member of this cluster are partners. Looking at the links among them the members of this cluster (green nodes) one can easily see how dense the connections in this cluster are. (A cluster of owners in Boston and Hartford offices).\n",
    "\n",
    "Another interesting phenomenon in this cluster is considering node number 13 in this cluster which show the meaningfulness of the clustering result because in spite of joining to the company very recently but she is one the Lawyers with partner status who has sufficient links with other Lawyers in Hartford office. \\qed\n",
    "\n",
    "The most contributing feature in the third cluster is office location. Moreover, Except node number 46, of all members of this cluster, are corporate attorneys. Though, since the members of this cluster, do not have strong connections among themselves and between the lawyers of other clusters, consequently, the link between node number 46 and node number 43, and the connection between 43 and 45 could be the reason of considering this node in the third cluster. \\qed  \n",
    "\n",
    "(think assging more weight to links should seperate this cluster into two clusters because of office feature in this cluter.)\n",
    "\n",
    "Office, Status, Law School and practice are sorted in descending order regarding their contribution to cluster number four. To be more precise, This cluster is a group of male litigation attorneys who work in a partnership framework. All of them expect node number 35, work in Hartford office. Looking at graphs structure, blue nodes can justify the reason of considering node number 35 in this cluster -- because it has links with node number 30 which is a kind of hub node in this cluster. \\qed\n",
    "\n",
    "The fifth cluster can be considered as a very tight cluster of male corporate lawyers of Boston office with practically the same age and the same years of experience of working in the firm. The nodes of this cluster are demonstrated with Cyan colour. \\qed\n",
    "\n",
    "\n",
    "This cluster consists of a man of the age 34 and a woman of age 53. Their working status is partner, they both work in Boston office and both are litigation attorneys who have stron connectios with different Lawyers of different cluster and with themselves as well. Moreover, they have practically the same years of experience in the firm. Nodes of this cluster are shown with olive colour. \\qed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Applying CESNA on this data set\n",
    "#### Loading the save results and other post processing methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/Soroosh/snap/examples/cesna/lawyers/Lawyers\"+ str(0) +\"cmtyvv.txt\", 'r') as fp:\n",
    "#     cesna = fp.readlines()\n",
    "\n",
    "# cesna_indices = []\n",
    "# for i in cesna:\n",
    "#     cesna_indices.append([int(ii) for ii in i.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "results of applying CESNA: ari: 0.28 nmi: 0.49\n",
      "Final: 0.28 0.00 0.49 0.00\n"
     ]
    }
   ],
   "source": [
    "ari_cesna_total, nmi_cesna_total = [], []\n",
    "\n",
    "for i in range(10):\n",
    "    with open(\"/home/Soroosh/snap/examples/cesna/lawyers/Lawyers\"+ str(i) +\"cmtyvv.txt\", 'r') as fp:\n",
    "        cesna = fp.readlines()\n",
    "        \n",
    "    cesna_indices = []\n",
    "    for i in cesna:\n",
    "        cesna_indices.append([int(ii) for ii in i.split()])\n",
    "\n",
    "    cesna_labels = np.zeros([N])\n",
    "    label = 1\n",
    "    for lst in cesna_indices:\n",
    "        for l in lst:\n",
    "            cesna_labels[l-1] = label\n",
    "        label += 1\n",
    "\n",
    "    ari_cesna = metrics.adjusted_rand_score(labels_true=labels_true_final, labels_pred=cesna_labels)\n",
    "    ami_cesna = metrics.adjusted_mutual_info_score(labels_true=labels_true_final, labels_pred=cesna_labels)\n",
    "    nmi_cesna = metrics.normalized_mutual_info_score(labels_true=labels_true_final, labels_pred=cesna_labels)\n",
    "\n",
    "    print(\"results of applying CESNA:\", \"ari:\", '%.2f' % ari_cesna,  \"nmi:\", '%0.2f' % nmi_cesna)\n",
    "    ari_cesna_total.append(ari_cesna)\n",
    "    nmi_cesna_total.append(nmi_cesna)\n",
    "    \n",
    "print(\"Final:\", \"%.2f\" % np.mean(ari_cesna_total), \"%.2f\" % np.std(ari_cesna_total), \"%.2f\" % np.mean(nmi_cesna_total), \"%.2f\" % np.std(nmi_cesna_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ari_cesna_total, nmi_cesna_total = [], []\n",
    "\n",
    "# for i in range(10):\n",
    "    \n",
    "#     with open(\"/home/Soroosh/snap/examples/cesna/lawyers/Lawyers\"+ str(i) +\"cmtyvv.txt\", 'r') as fp:\n",
    "#         cesna = fp.readlines()\n",
    "\n",
    "#     cesna_indices = []\n",
    "#     for i in cesna:\n",
    "#         cesna_indices.append([int(ii) for ii in i.split()])\n",
    "\n",
    "#     cesna_indices.sort(key=len, reverse=True)\n",
    "#     cesna_indices_sorted = deepcopy(cesna_indices)\n",
    "#     cesna_results_info = []\n",
    "#     c = 1\n",
    "#     for i in cesna_indices_sorted:\n",
    "#         cesna_results_info.append((c, len(i)))\n",
    "#         c += 1\n",
    "#     cesna_results_info\n",
    "\n",
    "\n",
    "#     cesna_results_unique_indices = []\n",
    "#     for i in range(len(cesna_indices_sorted)):\n",
    "#         tmp =cesna_indices_sorted[i]\n",
    "#         for j in range(i):\n",
    "#             tmp = set(tmp).difference(cesna_indices_sorted[j])\n",
    "#         cesna_results_unique_indices.append(list(tmp))\n",
    "\n",
    "\n",
    "\n",
    "#     diff = []  # add those nodes which are ignored by cesna \n",
    "#     cesna_indices_sorted_flat = [item for sublist in cesna_results_unique_indices for item in sublist]\n",
    "#     for i in range(1, N+1):\n",
    "#         if not i in cesna_indices_sorted_flat:\n",
    "#             diff += [i]\n",
    "\n",
    "# #     print(list(set(cesna_indices_sorted_flat)), len(list(set(cesna_indices_sorted_flat))))\n",
    "# #     print(\" \")\n",
    "# #     print(\"diff:\", diff, len(diff))\n",
    "\n",
    "\n",
    "#     cesna_labels = np.zeros([N])\n",
    "#     label = 0\n",
    "#     for lst in cesna_results_unique_indices:\n",
    "#         label += 1\n",
    "#         for l in lst:\n",
    "#             cesna_labels[l-1] = label\n",
    "\n",
    "# #     print(\"cesna_labels:\", len(cesna_labels))\n",
    "\n",
    "\n",
    "#     ##############################################################################################################\n",
    "#     # Remove/Add some elemenets from the output of CESNA results to compensate that\n",
    "#     # 10% of nodes which explained in section \"Chososing the number of communities\"\n",
    "#     ##############################################################################################################\n",
    "\n",
    "#     # diff_true_pred_labels = len(labels_true_final) - len(cesna_labels) \n",
    "#     # print(\"diff_pred_true_labels:\", diff_true_pred_labels)\n",
    "\n",
    "#     # # if diff_true_pred_labels > 0:\n",
    "#     # #     for d in diff_true_pred_labels:\n",
    "#     #         cesna_labels[d-1] = 50\n",
    "#     # # elif diff_true_pred_labels < 0:\n",
    "#     # #         cesna_labels.pop()\n",
    "\n",
    "#     cesna_labels = cesna_labels.tolist()\n",
    "# #     print(len(cesna_labels))\n",
    "\n",
    "\n",
    "#     ari_cesna = metrics.adjusted_rand_score(labels_true=labels_true_final, labels_pred=cesna_labels)\n",
    "#     ami_cesna = metrics.adjusted_mutual_info_score(labels_true=labels_true_final, labels_pred=cesna_labels)\n",
    "#     nmi_cesna = metrics.normalized_mutual_info_score(labels_true=labels_true_final, labels_pred=cesna_labels)\n",
    "\n",
    "#     print(\"results of applying CESNA:\", \"ari:\", '%.2f' % ari_cesna,  \"nmi:\", '%0.2f' % nmi_cesna)\n",
    "#     ari_cesna_total.append(ari_cesna)\n",
    "#     nmi_cesna_total.append(nmi_cesna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( \"%.2f\" % np.mean(ari_cesna_total), \"%.2f\" % np.std(ari_cesna_total), \"%.2f\" % np.mean(nmi_cesna_total), \"%.2f\" % np.std(nmi_cesna_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.27(48); 0.25(43); 0.25(0.43); 0.25(43); 0.28(43); 0.29(44); 0.29(44); 0.32(48) (-sb= 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data In GML format (for SIAN and etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating Nodes' attribute dict of dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):    for i in range(10):    for i in range(10):    # array2list = []\n",
    "# for i in range(N):\n",
    "#     array2list.append(list(Yc_unique[i, 1:]))\n",
    "\n",
    "# unique_features = []\n",
    "# for i in array2list:\n",
    "#     if not i in unique_features:\n",
    "#         unique_features.append(i)\n",
    "\n",
    "# # print(\"unique_features:\", unique_features)\n",
    "\n",
    "# labels_comb_man = [\"meta\" + str(i+1) for i in range(len(unique_features))]\n",
    "# labels_comb_man\n",
    "\n",
    "# Ggml = nx.from_numpy_array(Pin)\n",
    "\n",
    "# attributes_dict = {}\n",
    "\n",
    "# label = 1\n",
    "# labels_true = []\n",
    "# for uf in unique_features:  #UniqueFeatures\n",
    "#     tmp_labels = []\n",
    "#     for i in range(N):\n",
    "#         if uf == array2list[i]:\n",
    "#             attributes_dict[i] = {}\n",
    "#             key = labels_comb_man[label-1]\n",
    "#             attributes_dict[i][key] = key\n",
    "#             tmp_labels.append(label)\n",
    "#     label += 1\n",
    "\n",
    "# nx.set_node_attributes(Ggml, attributes_dict)\n",
    "# nx.write_gml(G=Ggml, path=\"../../data/Lawyers/Lawyers_SIAN.gml\")\n",
    "\n",
    "# with open (\"../../data/Lawyers/Lawyers_SIAN.gml\", 'r') as fp:\n",
    "#     GML = fp.readlines()\n",
    "# len(GML)\n",
    "\n",
    "# with open (\"../../../Newman_2016/Newman_Clauset_code/Lawyers.gml\", 'w') as fp:\n",
    "#     for i in range(len(GML)):\n",
    "#         if i ==3:\n",
    "#             fp.write(GML[0])\n",
    "#             fp.write(GML[1])\n",
    "#             fp.write(GML[2])\n",
    "#             fp.write(\"    \")\n",
    "#             fp.write(GML[3].split()[0])\n",
    "#             fp.write(\" \")\n",
    "#             fp.write(GML[4].split()[1])\n",
    "#             fp.write(\"\\n\")\n",
    "#             fp.write(GML[5])\n",
    "#         elif \"label\" in GML[i] and i!=3:\n",
    "#             fp.write(GML[i-2])\n",
    "#             fp.write(GML[i-1])\n",
    "#             fp.write(\"    \")\n",
    "#             fp.write(GML[i].split()[0])\n",
    "#             fp.write(\" \")\n",
    "#             fp.write(GML[i+1].split()[1])\n",
    "#             fp.write(\"\\n\")\n",
    "#             fp.write(GML[i+2])\n",
    "#         elif \"edge\" in GML[i]:\n",
    "#             fp.write(GML[i])\n",
    "#             fp.write(GML[i+1])\n",
    "#             fp.write(GML[i+2])\n",
    "#             fp.write(GML[i+4])\n",
    "\n",
    "\n",
    "# print(\"Conversion is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the SIAN's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of applying SIAN: ari: 0.58 nmi: 0.69\n",
      "results of applying SIAN: ari: 0.66 nmi: 0.75\n",
      "results of applying SIAN: ari: 0.60 nmi: 0.70\n",
      "results of applying SIAN: ari: 0.60 nmi: 0.70\n",
      "results of applying SIAN: ari: 0.61 nmi: 0.77\n",
      "results of applying SIAN: ari: 0.67 nmi: 0.76\n",
      "results of applying SIAN: ari: 0.54 nmi: 0.72\n",
      "results of applying SIAN: ari: 0.57 nmi: 0.68\n",
      "results of applying SIAN: ari: 0.57 nmi: 0.68\n",
      "results of applying SIAN: ari: 0.54 nmi: 0.62\n"
     ]
    }
   ],
   "source": [
    "ari_sian_total, nmi_sian_total = [], []\n",
    "for i in range(10):\n",
    "    with open (\"/home/Soroosh/Newman_2016/Newman_Clauset_code/lawyers/SIAN_Lawyers_results\" + str(i) + \".txt\", 'r') as fp:\n",
    "        SIAN = fp.readlines()\n",
    "\n",
    "    # labels_true_sorted_final = [item for sublist in labels_true_sorted for item in sublist]\n",
    "    sian_labels = []\n",
    "    for line in range(len(SIAN)):\n",
    "        tmp = SIAN[line].split()\n",
    "        prob = [float(tmp[t]) for t in range(len(tmp))  if t > 1]\n",
    "        sian_labels.append(np.argmax(np.asarray(prob)))\n",
    "    ari_sian = metrics.adjusted_rand_score(labels_true=labels_true_final, labels_pred=sian_labels)\n",
    "    ami_sian = metrics.adjusted_mutual_info_score(labels_true=labels_true_final, labels_pred=sian_labels)\n",
    "    nmi_sian = metrics.normalized_mutual_info_score(labels_true=labels_true_final, labels_pred=sian_labels)\n",
    "    ari_sian_total.append(ari_sian)\n",
    "    nmi_sian_total.append(nmi_sian)\n",
    "    print(\"results of applying SIAN:\", \"ari:\", '%.2f' % ari_sian,  \"nmi:\", '%0.2f' % nmi_sian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59 0.04 0.71 0.04\n"
     ]
    }
   ],
   "source": [
    "print( \"%.2f\" % np.mean(ari_sian_total), \"%.2f\" % np.std(ari_sian_total), \"%.2f\" % np.mean(nmi_sian_total), \"%.2f\" % np.std(nmi_sian_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
